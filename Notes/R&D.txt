| Pandas (Pros)                                 | SQL (Pros)                                   |
|-----------------------------------------------|----------------------------------------------|
| User-friendly syntax                          | Optimal performance for large datasets       |
| Excellent for exploratory data analysis       | Efficient querying and indexing              |
| Seamless integration with Python              | Robust data integrity and consistency        |

| Pandas (Cons)                                | SQL (Cons)                                  |
|-----------------------------------------------|----------------------------------------------|
| Slower performance for large datasets         | Steeper learning curve                      |
| Limited support for complex joins             | Syntax variations across database systems    |


Common Errors and Solutions
Pandas Common Errors
Memory Issues: Handling large datasets in Pandas may lead to memory errors. To address this, consider processing data in chunks or using more memory-efficient data types.

Inefficient Iteration: Iterating over rows in a DataFrame can be slow. Utilize vectorized operations in Pandas to enhance performance.

SQL Common Errors
Poorly Optimized Queries: Unoptimized SQL queries can be a bottleneck. Ensure your queries are well-structured, and consider indexing columns frequently used in search conditions.

Incorrect Joins: Misusing or omitting join conditions can lead to incorrect results. Double-check your join statements and ensure they match your data relationships.

#############################################################################################################################

**In SQL, the GROUP BY clause is used to group rows that have the same values in one or more columns into summary rows, while the HAVING clause is used to filter these grouped rows based on a specified condition. The WHERE clause filters individual rows before grouping, while the HAVING clause filters groups after they are formed.**

GROUP BY Clause:
Purpose: Groups rows based on the values in specified columns.
Syntax: GROUP BY column1, column2, ...
Usage: Used to aggregate data, calculate summary statistics for each group, and perform analysis on grouped data.
Example: SELECT department, AVG(salary) FROM employees GROUP BY department; (calculates the average salary for each department) 

HAVING Clause:
Purpose: Filters the grouped rows based on a condition.
Syntax: HAVING condition
Usage: Used to filter the results of a GROUP BY query, typically based on aggregate functions like SUM, AVG, COUNT, MAX, or MIN.
Example: SELECT department, AVG(salary) FROM employees GROUP BY department HAVING AVG(salary) > 50000; (selects departments with an average salary greater than 50000) 

Key Differences:
Filtering Scope: WHERE filters rows before grouping, while HAVING filters groups after grouping.
Aggregate Functions: HAVING can use aggregate functions in its condition, while WHERE cannot.
Order of Execution: WHERE is executed before GROUP BY, and HAVING is executed after GROUP BY. 

+-------------------------------------------------------------+
| Example:                                                    |
|                                                             |
| SELECT department, COUNT(*) as num_employees                |
| FROM employees                                              |
| WHERE salary > 30000                                        |
| GROUP BY department                                         |
| HAVING COUNT(*) > 5;                                        |
+-------------------------------------------------------------+

In this example: 
WHERE salary > 30000 filters out employees with salaries less than or equal to 30000.
GROUP BY department groups the remaining employees by department.
HAVING COUNT(*) > 5 filters the grouped departments, selecting only those with more than 5 employees.

#############################################################################################################################

etl_project/
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # DB and email settings
â”‚   â”œâ”€â”€ extract.py              # Extract CSV data
â”‚   â”œâ”€â”€ transform.py            # Clean and format data
â”‚   â”œâ”€â”€ load.py                 # Insert/Upsert to MySQL
â”‚   â””â”€â”€ alerts.py               # Email alert logic
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ etl.log                 # Auto-generated logs
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ new_sales_data.csv      # Source file
â”‚
â”œâ”€â”€ run_etl.py                  # Main driver script
â””â”€â”€ README.md

#############################################################################################################################

ðŸ” WHERE Clause
- Purpose: Filters individual rows before any grouping or aggregation.
- Used with: Regular columns (not aggregate functions).
- Example:
SELECT * FROM sales
WHERE region = 'West' AND quantity > 10;
- âœ… Filters raw data before grouping.

ðŸ“¦ GROUP BY Clause
- Purpose: Groups rows that share the same values in specified columns.
- Used with: Aggregate functions like SUM(), AVG(), COUNT(), etc.
- Example:
SELECT region, SUM(sales) FROM sales
GROUP BY region;
- âœ… Organizes data into buckets for aggregation.

ðŸ§® HAVING Clause
- Purpose: Filters groups after aggregation.
- Used with: Aggregate functions.
- Example:
SELECT region, SUM(sales) AS total_sales
FROM sales
GROUP BY region
HAVING SUM(sales) > 50000;
- âœ… Applies conditions to grouped results.

ðŸ§  Execution Order (Behind the Scenes)
- FROM
- WHERE
- GROUP BY
- HAVING
- SELECT
- ORDER BY
This order explains why HAVING can use aggregate functions but WHERE cannot.

#############################################################################################################################


The COALESCE operator is a super handy SQL function when you're dealing with missing or NULL values. It returns the first non-null value from a list of arguments

#############################################################################################################################

Transformation operations are map, filter, flatMap, groupByKey, reduceByKey, join, union, sortByKey, distinct, sample, mapPartitions, and aggregateByKey. These functions transform RDDs by applying computations in a distributed manner across a cluster of machines and return a new RDD

RDD actions in PySpark trigger computations and return results to the Spark driver. Key actions include collect, count, take, reduce, foreach, first, takeOrdered, takeSample, countByKey, saveAsTextFile, saveAsSequenceFile, saveAsObjectFile, foreachPartition, collectAsMap, aggregate, and fold.

#############################################################################################################################

What are PySpark serializers?
The serialization process is used to conduct performance tuning on Spark. The data sent or received over the network to the disk or memory should be persisted. PySpark supports serializers for this purpose. It supports two types of serializers, they are:

PickleSerializer: This serializes objects using Pythonâ€™s PickleSerializer (class pyspark.PickleSerializer). This supports almost every Python object.
MarshalSerializer: This performs serialization of objects. We can use it by using class pyspark.MarshalSerializer. This serializer is faster than the PickleSerializer but it supports only limited types.
Consider an example of serialization which makes use of MarshalSerializer:

# --serializing.py----
from pyspark.context import SparkContext
from pyspark.serializers import MarshalSerializer
sc = SparkContext("local", "Marshal Serialization", serializer = MarshalSerializer())    #Initialize spark context and serializer
print(sc.parallelize(list(range(1000))).map(lambda x: 3 * x).take(5))
sc.stop()

#############################################################################################################################

âœ… What is PySpark Streaming?
PySpark Streaming is a component of Apache Spark that enables real-time stream processing of live data streams using the Python API.

Instead of processing data in batches (like traditional ETL), it processes incoming data in small time intervals (called micro-batches).

#############################################################################################################################

ðŸ” What is Broadcast in PySpark?
In PySpark, a broadcast variable is a read-only shared variable that is cached on each worker node, instead of being sent with tasks. It's used to efficiently distribute small datasets across all worker nodes to avoid shuffling them multiple times.

ðŸ’¡ Why is it used?
When you perform joins between:

a large DataFrame (e.g. sales data with millions of rows)

and a small lookup table (e.g. country code mapping with 5 rows),

broadcasting the small table avoids data shuffling and improves performance significantly.

#############################################################################################################################

 1. What is Kafka and How It Works?
ðŸ”¸ What is Apache Kafka?
Apache Kafka is a distributed streaming platform used to build real-time data pipelines and streaming applications. It is designed to handle high-throughput, fault-tolerant, publish-subscribe messaging.

ðŸ”¸ Key Concepts:
| Component       | Description                                                        |
|-----------------|--------------------------------------------------------------------|
| Producer        | Sends data (messages) to Kafka topics.                             |
| Consumer        | Reads data from Kafka topics.                                      |
| Broker          | Kafka server that stores and serves messages.                      |
| Topic           | A named channel to which records are sent by producers and consumed by consumers. |
| Partition       | Topics are split into partitions for parallelism and scalability.  |
| Consumer Group  | A group of consumers working together to consume data from a topic.|
| Offset          | The position of a record in a partition (like a bookmark).         |

ðŸ”¸ How Kafka Works:
Producer sends messages to a topic.

Kafka stores those messages in partitions across brokers.

Consumers read messages from topics (one or more partitions).

Messages are retained (not deleted after consumption) for a configurable time (default 7 days).

Kafka is distributed, so it scales horizontally and is fault-tolerant via replication.

ðŸ”„ Flow Summary:

Producer â†’ Kafka Topic (Partitioned) â†’ Consumer
          (via Brokers & Storage)
âœ… 2. How Is Data Processed in Batches?
ðŸ”¸ Batch Processing (in Streaming Context):
Even in streaming systems like Kafka, batch processing is often used internally to improve performance and resource utilization. Here's how it works:

âš™ï¸ In Kafka â†’ Pandas use case:
Batch Size: Kafka consumer can be configured to poll multiple records at once using consumer.poll(timeout_ms, max_records=N).

Buffering: Messages are buffered in memory.

Pandas: Once a batch is collected (say 100 messages), convert it into a Pandas DataFrame.

Transformation: Apply cleaning, parsing, or business logic in bulk using Pandas operations.

SQL Write: Write the entire batch to SQL using bulk inserts or to_sql() with chunksize.

ðŸ”¸ Benefits of Batching in Streaming:
Benefit	-->                 Why It Matters
Efficiency -->	            Reduced I/O overhead compared to record-by-record writes.
Throughput -->	            Increases overall message processing rate.
Resource Optimization -->	Fewer DB connections, CPU cycles.


Key type	Purpose	Characteristics
Primary Key	Used to uniquely identify a row in a table	Cannot be NULL, and must be a unique one per table
Foreign Key	Used to maintain referential integrity between tables	It can be NULL
Composite Key	Used to uniquely identify a row when a single column is not sufficient	It is a combination of columns, however, they must be unique.
Unique Key	Used to prevent duplicate values in a column	It can be NULL
Candidate Key	Used to identify potential Primary Keys	It can be unique and can uniquely identify each row in a table
Super Key	Used to uniquely identify rows in a broad sense	It can contain additional non-unique columns

================================================================================================================================

***what is data skewness in spark?
-->
Data skewness in Spark refers to the situation where data is distributed unevenly across the partitions of a distributed dataset, leading to some partitions (and their associated tasks) containing much more data than others. This imbalance causes certain tasks to take significantly longer to complete, resulting in performance bottlenecks, inefficient resource utilization, and increased probability of job failures or out-of-memory errors.[1][2][4][5]

**Why it occurs:**
- Data skew typically arises during operations that require shuffling or grouping by certain columns (such as join, groupBy, or reduceByKey) where the distribution of keys is not uniformâ€”e.g., one key appears far more often than others.[2][3][4][1]
- An example is when a few customer IDs or product codes dominate the dataset, so partitions corresponding to those keys carry much more data than others.[2]

**Consequences:**
- Slow-running stages/tasks, because overloaded partitions take much longer to process.[3][4][5][1]
- Spilling to disk or out-of-memory errors on executors with large partitions.[5][9][1]
- Poor utilization of the cluster, as many nodes finish early and sit idle while skewed tasks complete.[9][5]

**Detection:**
- Use the Spark UI or execution plan to look for tasks or partitions that take much longer or process far more data compared to others.[1][2]
- Metrics such as max/min task duration or shuffle write/read size can highlight skewed partitionsâ€”large discrepancies are a sign of skewness.[4][2]

**Mitigation Techniques:**
- Repartitioning: Redistribute data more evenly using repartition() or coalesce() functions.[8][3][5][1]
- Salting: Add a random number or value to skewed keys to distribute them across multiple partitions, especially in join operations.[4][5][2]
- Broadcast joins: Broadcast a small table to all workers to prevent large shuffles.[5][4]
- Bucketing: Pre-group data into fixed-size buckets based on certain columns.[3][5]
- Adaptive Query Execution (AQE): Allows Spark to adjust execution plans at runtime to compensate for skewed data by splitting large tasks.[7][4]
- Custom partitioning: Implement a partitioning strategy tailored to the specific data distribution in your application.[5]

================================================================================================================================

