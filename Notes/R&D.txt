| Pandas (Pros)                                 | SQL (Pros)                                   |
|-----------------------------------------------|----------------------------------------------|
| User-friendly syntax                          | Optimal performance for large datasets       |
| Excellent for exploratory data analysis       | Efficient querying and indexing              |
| Seamless integration with Python              | Robust data integrity and consistency        |

| Pandas (Cons)                                | SQL (Cons)                                  |
|-----------------------------------------------|----------------------------------------------|
| Slower performance for large datasets         | Steeper learning curve                      |
| Limited support for complex joins             | Syntax variations across database systems    |


Common Errors and Solutions
Pandas Common Errors
Memory Issues: Handling large datasets in Pandas may lead to memory errors. To address this, consider processing data in chunks or using more memory-efficient data types.

Inefficient Iteration: Iterating over rows in a DataFrame can be slow. Utilize vectorized operations in Pandas to enhance performance.

SQL Common Errors
Poorly Optimized Queries: Unoptimized SQL queries can be a bottleneck. Ensure your queries are well-structured, and consider indexing columns frequently used in search conditions.

Incorrect Joins: Misusing or omitting join conditions can lead to incorrect results. Double-check your join statements and ensure they match your data relationships.

#############################################################################################################################

**In SQL, the GROUP BY clause is used to group rows that have the same values in one or more columns into summary rows, while the HAVING clause is used to filter these grouped rows based on a specified condition. The WHERE clause filters individual rows before grouping, while the HAVING clause filters groups after they are formed.**

GROUP BY Clause:
Purpose: Groups rows based on the values in specified columns.
Syntax: GROUP BY column1, column2, ...
Usage: Used to aggregate data, calculate summary statistics for each group, and perform analysis on grouped data.
Example: SELECT department, AVG(salary) FROM employees GROUP BY department; (calculates the average salary for each department) 

HAVING Clause:
Purpose: Filters the grouped rows based on a condition.
Syntax: HAVING condition
Usage: Used to filter the results of a GROUP BY query, typically based on aggregate functions like SUM, AVG, COUNT, MAX, or MIN.
Example: SELECT department, AVG(salary) FROM employees GROUP BY department HAVING AVG(salary) > 50000; (selects departments with an average salary greater than 50000) 

Key Differences:
Filtering Scope: WHERE filters rows before grouping, while HAVING filters groups after grouping.
Aggregate Functions: HAVING can use aggregate functions in its condition, while WHERE cannot.
Order of Execution: WHERE is executed before GROUP BY, and HAVING is executed after GROUP BY. 

+-------------------------------------------------------------+
| Example:                                                    |
|                                                             |
| SELECT department, COUNT(*) as num_employees                |
| FROM employees                                              |
| WHERE salary > 30000                                        |
| GROUP BY department                                         |
| HAVING COUNT(*) > 5;                                        |
+-------------------------------------------------------------+

In this example: 
WHERE salary > 30000 filters out employees with salaries less than or equal to 30000.
GROUP BY department groups the remaining employees by department.
HAVING COUNT(*) > 5 filters the grouped departments, selecting only those with more than 5 employees.

#############################################################################################################################

etl_project/
‚îÇ
‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # DB and email settings
‚îÇ   ‚îú‚îÄ‚îÄ extract.py              # Extract CSV data
‚îÇ   ‚îú‚îÄ‚îÄ transform.py            # Clean and format data
‚îÇ   ‚îú‚îÄ‚îÄ load.py                 # Insert/Upsert to MySQL
‚îÇ   ‚îî‚îÄ‚îÄ alerts.py               # Email alert logic
‚îÇ
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ etl.log                 # Auto-generated logs
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ new_sales_data.csv      # Source file
‚îÇ
‚îú‚îÄ‚îÄ run_etl.py                  # Main driver script
‚îî‚îÄ‚îÄ README.md

#############################################################################################################################

üîç WHERE Clause
- Purpose: Filters individual rows before any grouping or aggregation.
- Used with: Regular columns (not aggregate functions).
- Example:
SELECT * FROM sales
WHERE region = 'West' AND quantity > 10;
- ‚úÖ Filters raw data before grouping.

üì¶ GROUP BY Clause
- Purpose: Groups rows that share the same values in specified columns.
- Used with: Aggregate functions like SUM(), AVG(), COUNT(), etc.
- Example:
SELECT region, SUM(sales) FROM sales
GROUP BY region;
- ‚úÖ Organizes data into buckets for aggregation.

üßÆ HAVING Clause
- Purpose: Filters groups after aggregation.
- Used with: Aggregate functions.
- Example:
SELECT region, SUM(sales) AS total_sales
FROM sales
GROUP BY region
HAVING SUM(sales) > 50000;
- ‚úÖ Applies conditions to grouped results.

üß† Execution Order (Behind the Scenes)
- FROM
- WHERE
- GROUP BY
- HAVING
- SELECT
- ORDER BY
This order explains why HAVING can use aggregate functions but WHERE cannot.

#############################################################################################################################


The COALESCE operator is a super handy SQL function when you're dealing with missing or NULL values. It returns the first non-null value from a list of arguments

#############################################################################################################################

Transformation operations are map, filter, flatMap, groupByKey, reduceByKey, join, union, sortByKey, distinct, sample, mapPartitions, and aggregateByKey. These functions transform RDDs by applying computations in a distributed manner across a cluster of machines and return a new RDD

RDD actions in PySpark trigger computations and return results to the Spark driver. Key actions include collect, count, take, reduce, foreach, first, takeOrdered, takeSample, countByKey, saveAsTextFile, saveAsSequenceFile, saveAsObjectFile, foreachPartition, collectAsMap, aggregate, and fold.

#############################################################################################################################

What are PySpark serializers?
The serialization process is used to conduct performance tuning on Spark. The data sent or received over the network to the disk or memory should be persisted. PySpark supports serializers for this purpose. It supports two types of serializers, they are:

PickleSerializer: This serializes objects using Python‚Äôs PickleSerializer (class pyspark.PickleSerializer). This supports almost every Python object.
MarshalSerializer: This performs serialization of objects. We can use it by using class pyspark.MarshalSerializer. This serializer is faster than the PickleSerializer but it supports only limited types.
Consider an example of serialization which makes use of MarshalSerializer:

# --serializing.py----
from pyspark.context import SparkContext
from pyspark.serializers import MarshalSerializer
sc = SparkContext("local", "Marshal Serialization", serializer = MarshalSerializer())    #Initialize spark context and serializer
print(sc.parallelize(list(range(1000))).map(lambda x: 3 * x).take(5))
sc.stop()

#############################################################################################################################

‚úÖ What is PySpark Streaming?
PySpark Streaming is a component of Apache Spark that enables real-time stream processing of live data streams using the Python API.

Instead of processing data in batches (like traditional ETL), it processes incoming data in small time intervals (called micro-batches).

#############################################################################################################################

üîç What is Broadcast in PySpark?
In PySpark, a broadcast variable is a read-only shared variable that is cached on each worker node, instead of being sent with tasks. It's used to efficiently distribute small datasets across all worker nodes to avoid shuffling them multiple times.

üí° Why is it used?
When you perform joins between:

a large DataFrame (e.g. sales data with millions of rows)

and a small lookup table (e.g. country code mapping with 5 rows),

broadcasting the small table avoids data shuffling and improves performance significantly.